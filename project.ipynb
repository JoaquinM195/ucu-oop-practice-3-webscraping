{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCU - Proyecto Webscraping\n",
    "Proyecto para la Licenciatura en Datos y Negocios de la Universidad Católica del Uruguay"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T23:46:25.524198Z",
     "start_time": "2025-09-18T23:46:11.736802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Montevideo (prop.com.uy) ===\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "mvd_props = []\n",
    "base_url = 'https://prop.com.uy'\n",
    "list_urls = [\n",
    "    base_url + '/propiedades/comprar',   # listado típico\n",
    "    base_url + '/propiedades',           # alternativa\n",
    "    base_url                              # fallback\n",
    "]\n",
    "\n",
    "for page in range(1, 8):  # subí este rango si necesitás más resultados\n",
    "    candidate_pages = [f'{u}?page={page}' for u in list_urls] + [list_urls[-1]]\n",
    "    got_page = False\n",
    "\n",
    "    for url in candidate_pages:\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "            if resp.status_code != 200 or not resp.text:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            containers = soup.find_all('a', href=lambda h: h and (\n",
    "                '/propiedades/' in h or '/propiedad/' in h\n",
    "            ))\n",
    "\n",
    "            seen, links = set(), []\n",
    "            for a in containers:\n",
    "                href = a.get('href')\n",
    "                if not href:\n",
    "                    continue\n",
    "                full = href if href.startswith('http') else (base_url + href)\n",
    "                if full not in seen:\n",
    "                    seen.add(full)\n",
    "                    links.append(full)\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    r = requests.get(link, headers=HEADERS, timeout=20)\n",
    "                    s = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "                    price_txt = s.find(string=lambda t: t and ('U$S' in t or 'USD' in t or '$' in t))\n",
    "                    rooms_txt = s.find(string=lambda t: t and ('Dorm' in t or 'dorm' in t or 'Dormitorio' in t))\n",
    "                    size_txt  = s.find(string=lambda t: t and ('m²' in t or 'm2' in t or 'M2' in t))\n",
    "\n",
    "                    # --- normalización de dormitorios ---\n",
    "                    habitaciones = None\n",
    "                    if rooms_txt:\n",
    "                        m = re.search(r'(\\d+)', rooms_txt)\n",
    "                        if m:\n",
    "                            n = int(m.group(1))\n",
    "                            if n == 1:\n",
    "                                habitaciones = \"1 dormitorio\"\n",
    "                            else:\n",
    "                                habitaciones = f\"{n} dormitorios\"\n",
    "\n",
    "                    mvd_props.append({\n",
    "                        'precio': price_txt.strip() if price_txt else None,\n",
    "                        'tamano': size_txt.strip() if size_txt else None,\n",
    "                        'habitaciones': habitaciones,\n",
    "                        'link': link\n",
    "                    })\n",
    "\n",
    "                    if len(mvd_props) >= 10:\n",
    "                        break\n",
    "                except requests.exceptions.RequestException:\n",
    "                    continue\n",
    "\n",
    "            got_page = True\n",
    "            if len(mvd_props) >= 10:\n",
    "                break\n",
    "        except requests.exceptions.RequestException:\n",
    "            continue\n",
    "\n",
    "    if len(mvd_props) >= 10:\n",
    "        break\n",
    "    if not got_page:\n",
    "        continue\n",
    "\n",
    "# guardado en JSON\n",
    "data = {\n",
    "    \"ciudades\": [\n",
    "        {\n",
    "            \"nombre\": \"Montevideo\",\n",
    "            \"propiedades\": mvd_props[:10]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"propiedades.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Montevideo listo → propiedades.json\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montevideo listo → propiedades.json\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T23:45:42.702532Z",
     "start_time": "2025-09-18T23:44:28.685522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Salto (Durán & Piastri) — llegar a ≥10 propiedades con paginación amplia ===\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "base_url = \"https://www.duranypiastri.com\"\n",
    "\n",
    "# listado principal de venta + variantes de paginado (intentamos varias por si el sitio usa nombres distintos)\n",
    "list_patterns = [\n",
    "    \"/?op=1&pag=propiedades\",                         # base\n",
    "    \"/?op=1&pag=propiedades&page={page}\",            # variante 1\n",
    "    \"/?op=1&pag=propiedades&pagina={page}\",          # variante 2\n",
    "    \"/?op=1&pag=propiedades&p={page}\",               # variante 3\n",
    "    \"/?op=1&pag=propiedades&pg={page}\",              # variante 4\n",
    "    \"/?page={page}\",                                  # fallback genérico\n",
    "    \"/?pagina={page}\"                                 # fallback genérico\n",
    "]\n",
    "\n",
    "DESIRED = 10\n",
    "MAX_PAGES = 30\n",
    "\n",
    "def normalize_link(base, href):\n",
    "    if not href:\n",
    "        return None\n",
    "    href = href.strip()\n",
    "    if href.startswith((\"javascript:\", \"#\")):\n",
    "        return None\n",
    "    if href.startswith((\"http://\", \"https://\")):\n",
    "        return href\n",
    "    if href.startswith(\"/\"):\n",
    "        return base + href\n",
    "    return base.rstrip(\"/\") + \"/\" + href\n",
    "\n",
    "def extract_price_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    t = text.replace(\"\\xa0\", \" \").replace(\"\\n\", \" \")\n",
    "    m = re.search(r\"Venta\\s*(U\\$S|USD|\\$)\\s*[\\d\\.\\,]+\", t, re.I)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    m = re.search(r\"(U\\$S|USD|\\$)\\s*[\\d\\.\\,]+\", t, re.I)\n",
    "    return m.group(0).strip() if m else None\n",
    "\n",
    "def extract_rooms_fmt(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    t = text.replace(\"\\xa0\",\" \").replace(\"\\n\",\" \")\n",
    "    m = re.search(r\"(\\d+)\\s*Dormitorio/?s?\", t, re.I)\n",
    "    if not m:\n",
    "        m = re.search(r\"(\\d+)\\s*Dorms?\\.?\", t, re.I)\n",
    "    if m:\n",
    "        n = int(m.group(1))\n",
    "        return \"1 dormitorio\" if n == 1 else f\"{n} dormitorios\"\n",
    "    return None\n",
    "\n",
    "def extract_m2(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    t = text.replace(\"\\xa0\", \" \").replace(\"\\n\", \" \")\n",
    "    m = re.search(r\"(\\d{1,4})\\s*(m²|m2|M2|mt2|mts2|m\\^2)\\b\", t, re.I)\n",
    "    return f\"{m.group(1)} m²\" if m else None\n",
    "\n",
    "salto_props = []\n",
    "seen_detail = set()   # para no repetir fichas\n",
    "\n",
    "# recorremos páginas hasta juntar DESIRED\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    found_this_page = False\n",
    "\n",
    "    for pat in list_patterns:\n",
    "        url = base_url + (pat.format(page=page) if \"{page}\" in pat else pat)\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "            if resp.status_code != 200 or not resp.text:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            # candidatos a ficha (patrones amplios)\n",
    "            anchors = soup.find_all(\"a\", href=lambda h: h and any(x in h for x in [\n",
    "                \"idprop=\", \"prop=\", \"/propiedad\", \"/propiedades\", \"ficha\"\n",
    "            ]))\n",
    "\n",
    "            # normalizamos y filtramos enlaces que aparentan ser fichas (evitamos volver a listados)\n",
    "            detail_links = []\n",
    "            for a in anchors:\n",
    "                full = normalize_link(base_url, a.get(\"href\"))\n",
    "                if not full:\n",
    "                    continue\n",
    "                # evitamos enlaces al propio listado\n",
    "                if \"pag=propiedades\" in full and \"idprop=\" not in full and \"prop=\" not in full and \"/propiedad\" not in full:\n",
    "                    continue\n",
    "                if full not in seen_detail:\n",
    "                    seen_detail.add(full)\n",
    "                    detail_links.append(full)\n",
    "\n",
    "            # parseamos fichas\n",
    "            for link in detail_links:\n",
    "                try:\n",
    "                    r = requests.get(link, headers=HEADERS, timeout=20)\n",
    "                    if r.status_code != 200 or not r.text:\n",
    "                        continue\n",
    "                    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "                    full = s.get_text(\" \", strip=True)\n",
    "\n",
    "                    price = extract_price_text(full)\n",
    "                    rooms = extract_rooms_fmt(full)\n",
    "                    size  = extract_m2(full)\n",
    "\n",
    "                    salto_props.append({\n",
    "                        \"precio\": price,\n",
    "                        \"tamano\": size,\n",
    "                        \"habitaciones\": rooms,\n",
    "                        \"link\": link\n",
    "                    })\n",
    "                    found_this_page = True\n",
    "\n",
    "                    if len(salto_props) >= DESIRED:\n",
    "                        break\n",
    "                except requests.exceptions.RequestException:\n",
    "                    continue\n",
    "\n",
    "            if len(salto_props) >= DESIRED:\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            continue\n",
    "\n",
    "    if len(salto_props) >= DESIRED:\n",
    "        break\n",
    "    # si esta iteración de page no trajo nada, igual seguimos probando siguientes páginas/patrones\n",
    "\n",
    "# Guardado JSON en el formato pedido (una sola ciudad)\n",
    "data = {\n",
    "    \"ciudades\": [\n",
    "        {\"nombre\": \"Salto\", \"propiedades\": salto_props[:DESIRED]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"propiedades.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Salto listo → propiedades.json (total: {len(salto_props)})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salto listo → propiedades.json (total: 10)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T23:54:25.510367Z",
     "start_time": "2025-09-18T23:53:57.540635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Punta del Este (Market del Este) — enlaces correctos + datos normalizados ===\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\", \"Accept-Language\": \"es-UY,es;q=0.9,en;q=0.8\"}\n",
    "\n",
    "base_url = \"https://www.marketdeleste.com\"\n",
    "seed_url = base_url + \"/apartamentos/en-venta/?ref=&ciudad=2&dormitorios=&precioMaximo=\"\n",
    "\n",
    "DESIRED = 10\n",
    "MAX_PAGES = 50  # si necesitás más resultados, subilo\n",
    "\n",
    "# Variantes de paginado que suelen usar estos sitios\n",
    "list_patterns = [\n",
    "    seed_url,\n",
    "    seed_url + \"&page={page}\",\n",
    "    seed_url + \"&pagina={page}\",\n",
    "    seed_url + \"&paged={page}\",\n",
    "    base_url + \"/apartamentos/en-venta/page/{page}/\",\n",
    "]\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def abs_url(base, href):\n",
    "    if not href: return None\n",
    "    href = href.strip()\n",
    "    if href.startswith((\"javascript:\", \"#\")): return None\n",
    "    if href.startswith((\"http://\", \"https://\")): return href\n",
    "    if href.startswith(\"/\"): return base + href\n",
    "    return base.rstrip(\"/\") + \"/\" + href\n",
    "\n",
    "def is_list_url(url):\n",
    "    # Evitar que se cuelen listados/paginadores en el JSON\n",
    "    if not url: return True\n",
    "    bad_bits = [\"/en-venta?\", \"/en-venta/&\", \"/en-venta/#\", \"/en-venta/page/\", \"pagina=\", \"paged=\", \"page=\"]\n",
    "    return any(bit in url for bit in bad_bits)\n",
    "\n",
    "def price_text(text):\n",
    "    if not text: return None\n",
    "    t = text.replace(\"\\xa0\", \" \").replace(\"\\n\", \" \")\n",
    "    m = re.search(r\"(U\\$S|US\\$|USD|\\$)\\s*[\\d\\.\\,]+\", t, re.I)\n",
    "    return m.group(0).strip() if m else None\n",
    "\n",
    "def price_to_int_usd(text):\n",
    "    if not text: return None\n",
    "    # toma solo dígitos y comas/puntos, y convierte a int\n",
    "    digits = re.sub(r\"[^\\d]\", \"\", text)\n",
    "    try:\n",
    "        return int(digits) if digits else None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def rooms_fmt(text):\n",
    "    if not text: return None\n",
    "    t = text.replace(\"\\xa0\",\" \").replace(\"\\n\",\" \")\n",
    "    m = re.search(r\"(\\d+)\\s*Dorms?\\.?\", t, re.I)\n",
    "    if not m:\n",
    "        m = re.search(r\"(\\d+)\\s*Dormitorio/?s?\", t, re.I)\n",
    "    if m:\n",
    "        n = int(m.group(1))\n",
    "        return \"1 dormitorio\" if n == 1 else f\"{n} dormitorios\"\n",
    "    return None\n",
    "\n",
    "def size_m2(text):\n",
    "    if not text: return None\n",
    "    t = text.replace(\"\\xa0\", \" \").replace(\"\\n\", \" \")\n",
    "    m = re.search(r\"(\\d{1,4})\\s*(m²|m2|M2|mt2|mts2|m\\^2)\\b\", t, re.I)\n",
    "    return f\"{m.group(1)} m²\" if m else None\n",
    "\n",
    "def get_detail_fields(detail_html):\n",
    "    s = BeautifulSoup(detail_html, \"html.parser\")\n",
    "    full = s.get_text(\" \", strip=True)\n",
    "\n",
    "    p_txt = price_text(full)\n",
    "    p_num = price_to_int_usd(p_txt)\n",
    "    rooms = rooms_fmt(full)\n",
    "    m2 = size_m2(full)\n",
    "\n",
    "    return p_num, m2, rooms\n",
    "\n",
    "# ---------------- scraping ----------------\n",
    "props = []\n",
    "seen = set()  # dedupe por link\n",
    "\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    got_any = False\n",
    "\n",
    "    for pat in list_patterns:\n",
    "        url = pat.format(page=page) if \"{page}\" in pat else pat\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "            if resp.status_code != 200 or not resp.text:\n",
    "                continue\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            # Candidatos a ficha (enlaces con slugs típicos de detalle)\n",
    "            anchors = soup.find_all(\"a\", href=True)\n",
    "            cand_links = []\n",
    "            for a in anchors:\n",
    "                href = a.get(\"href\")\n",
    "                full = abs_url(base_url, href)\n",
    "                if not full:\n",
    "                    continue\n",
    "                # Filtramos listados y priorizamos detalle (propiedad/apartamento con slug)\n",
    "                if is_list_url(full):\n",
    "                    continue\n",
    "                if any(seg in full for seg in [\"/propiedad/\", \"/propiedades/\", \"/apartamento\", \"/apartamentos\"]):\n",
    "                    if full not in seen:\n",
    "                        seen.add(full)\n",
    "                        cand_links.append(full)\n",
    "\n",
    "            # Si no hubo suerte con filtros, relajamos (fallback): tomamos todos y filtramos por contenido de ficha\n",
    "            if not cand_links:\n",
    "                for a in anchors:\n",
    "                    full = abs_url(base_url, a.get(\"href\"))\n",
    "                    if full and full not in seen and not is_list_url(full):\n",
    "                        seen.add(full)\n",
    "                        cand_links.append(full)\n",
    "\n",
    "            # Abrimos cada candidato y validamos que sea ficha (y extraemos campos)\n",
    "            for link in cand_links:\n",
    "                try:\n",
    "                    r = requests.get(link, headers=HEADERS, timeout=20)\n",
    "                    if r.status_code != 200 or not r.text:\n",
    "                        continue\n",
    "\n",
    "                    price_num, m2_txt, rooms_str = get_detail_fields(r.text)\n",
    "\n",
    "                    # Consideramos \"ficha válida\" si recuperamos al menos precio numérico o m2 o dormitorios\n",
    "                    if price_num is None and m2_txt is None and rooms_str is None:\n",
    "                        continue  # probablemente no es ficha\n",
    "\n",
    "                    props.append({\n",
    "                        \"precio\": price_num,        # número en USD si se pudo parsear, si no -> None\n",
    "                        \"tamano\": m2_txt,           # 'NN m²' si se encontró\n",
    "                        \"habitaciones\": rooms_str,  # 'X dormitorio(s)' si se encontró\n",
    "                        \"link\": link                # absoluto y verificado\n",
    "                    })\n",
    "                    got_any = True\n",
    "\n",
    "                    if len(props) >= DESIRED:\n",
    "                        break\n",
    "                except requests.exceptions.RequestException:\n",
    "                    continue\n",
    "\n",
    "            if len(props) >= DESIRED:\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            continue\n",
    "\n",
    "    if len(props) >= DESIRED:\n",
    "        break\n",
    "    if not got_any:\n",
    "        continue\n",
    "\n",
    "# ---------------- guardado ----------------\n",
    "data = {\n",
    "    \"ciudades\": [\n",
    "        {\"nombre\": \"Punta del Este\", \"propiedades\": props[:DESIRED]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"propiedades.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Punta del Este listo → propiedades.json (total: {len(props)})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punta del Este listo → propiedades.json (total: 10)\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
