{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCU - Proyecto Webscraping\n",
    "Proyecto para la Licenciatura en Datos y Negocios de la Universidad Católica del Uruguay"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scraping de propiedades — Inmobiliaria Uruguay\n",
    "Este notebook descarga listados **públicos** desde *inmobiliariauruguay.com* usando `requests`\n",
    "y analiza el HTML con `BeautifulSoup` para extraer:\n",
    "\n",
    "- Ciudad (agrupado por **Departamento** en el sitio, p. ej. *Montevideo*, *Maldonado*, *Canelones*)\n",
    "- Precio (USD)\n",
    "- Tamaño (m², si está disponible)\n",
    "- Dormitorios (si está disponible)\n",
    "- Link al aviso\n",
    "\n",
    "Requisitos:\n",
    "- `requests`, `beautifulsoup4` (instalar con `pip install requests beautifulsoup4`)\n",
    "\n",
    "Comentario:\n",
    "- Si bien se utilizaron las herramientas sugeridas por la consigna, también se aplica el uso de otras que fueron aprendidas en videos y tutoriales visualizados para obtener un mejor resultado"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1) Imports y funciones auxiliares"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En este bloque se definen las librerías necesarias y funciones de apoyo para realizar el web scraping en el sitio **inmobiliariauruguay.com**.\n",
    "\n",
    "- **Imports:** Se cargan librerías como `re`, `json`, `time`, `requests` y `BeautifulSoup` para manejar expresiones regulares, trabajar con JSON, controlar tiempos, hacer peticiones HTTP y parsear el HTML.\n",
    "- **HEADERS y BASE:** Se configura un encabezado de navegador (User-Agent) para evitar bloqueos y se define la URL base del sitio.\n",
    "- **Sesión de requests:** Se crea una sesión HTTP con cabeceras personalizadas para reutilizar la conexión.\n",
    "- **Función `get_soup(url)`:** Realiza la petición al sitio, valida la respuesta y devuelve el HTML en formato `BeautifulSoup` para su análisis.\n",
    "- **Función `to_int(text)`:** Extrae números de un texto, eliminando puntos de miles y convirtiéndolos a enteros.\n",
    "- **Función `clean_m2(x)`:** Limpia textos de áreas eliminando `m2` o `m²`, y los transforma a enteros usando `to_int`.\n",
    "- **Función `parse_title_city(title)`:** A partir del título de un aviso, intenta separar el nombre de la ciudad (última parte luego de coma o guion).\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:00:13.425584Z",
     "start_time": "2025-09-19T02:00:13.417674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"es-ES,es;q=0.9\"\n",
    "}\n",
    "\n",
    "BASE = \"https://inmobiliariauruguay.com\"\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def get_soup(url):\n",
    "    resp = session.get(url, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "def to_int(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    nums = re.findall(r\"[0-9][0-9\\.]*\", text.replace(\"\\xa0\",\" \"))\n",
    "    if not nums:\n",
    "        return None\n",
    "    n = nums[0].replace(\".\", \"\")\n",
    "    try:\n",
    "        return int(n)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def clean_m2(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    x = x.replace(\"m2\", \"\").replace(\"m²\", \"\")\n",
    "    return to_int(x)\n",
    "\n",
    "def parse_title_city(title):\n",
    "    if not title:\n",
    "        return None\n",
    "    import re\n",
    "    parts = re.split(r\"[,–-]\", title)\n",
    "    last = parts[-1].strip() if parts else None\n",
    "    if last:\n",
    "        last = re.sub(r\"\\.$\", \"\", last)\n",
    "    return last or None"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2) Listado y paginación"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La función **`extract_cards_from_listing`** se encarga de recorrer los listados de propiedades y extraer la información principal de cada aviso.\n",
    "\n",
    "- **Parámetros:**\n",
    "  - `list_url`: URL inicial del listado de propiedades.\n",
    "  - `max_pages`: número máximo de páginas a recorrer (por defecto 5).\n",
    "\n",
    "- **Flujo de trabajo:**\n",
    "  1. Crea una lista `out` donde se guardarán los resultados.\n",
    "  2. Descarga y procesa la página con `get_soup`.\n",
    "  3. Busca los enlaces de propiedades dentro de etiquetas `h2 a` o `h3 a`.\n",
    "  4. Para cada tarjeta:\n",
    "     - Obtiene el **título** de la propiedad.\n",
    "     - Intenta encontrar el **precio** buscando hacia arriba en el árbol HTML hasta detectar un texto con `\"USD\"`.\n",
    "     - Construye un diccionario con `title`, `price_text` y la URL absoluta del detalle.\n",
    "  5. Verifica si existe un enlace de paginación con el número de la siguiente página (`page+1`). Si lo encuentra, actualiza `url` y continúa el bucle.\n",
    "  6. El proceso se repite hasta llegar al límite de páginas o cuando no hay más resultados.\n",
    "\n",
    "- **Salida:**\n",
    "  Devuelve una lista de diccionarios con los datos de cada propiedad encontrada en el listado.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:00:16.077963Z",
     "start_time": "2025-09-19T02:00:16.072247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_cards_from_listing(list_url, max_pages=5):\n",
    "    out = []\n",
    "    page = 1\n",
    "    url = list_url\n",
    "    while page <= max_pages and url:\n",
    "        soup = get_soup(url)\n",
    "        for card in soup.select(\"h2 a, h3 a\"):\n",
    "            href = card.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            title = card.get_text(strip=True)\n",
    "            price_el = None\n",
    "            candidate = card\n",
    "            for _ in range(5):\n",
    "                candidate = candidate.find_parent()\n",
    "                if not candidate: break\n",
    "                price_el = candidate.find(string=lambda s: isinstance(s, str) and \"USD\" in s)\n",
    "                if price_el: break\n",
    "            price_text = (price_el.strip() if isinstance(price_el, str) else price_el.get_text(strip=True)) if price_el else None\n",
    "            out.append({\n",
    "                \"title\": title,\n",
    "                \"price_text\": price_text,\n",
    "                \"detail_url\": urljoin(BASE, href)\n",
    "            })\n",
    "        next_link = None\n",
    "        for a in soup.select(\"a\"):\n",
    "            if a.get_text(strip=True) == str(page+1):\n",
    "                next_link = urljoin(BASE, a.get(\"href\"))\n",
    "        if next_link and next_link != url:\n",
    "            url = next_link\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3) Parser de la página de detalle"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La función **`parse_detail`** extrae la información detallada de una propiedad a partir de su página individual.\n",
    "\n",
    "- **Parámetros:**\n",
    "  - `detail_url`: enlace a la página de detalle de la propiedad.\n",
    "\n",
    "- **Flujo de trabajo:**\n",
    "  1. Descarga el HTML con `get_soup` y localiza el **título** en un `<h1>` o `<h2>`.\n",
    "  2. Inicializa variables para almacenar: **precio**, **superficie (m²)**, **número de dormitorios** y **ciudad**.\n",
    "  3. Recorre todos los elementos `<li>` de la página, creando un diccionario `detalles` con clave/valor a partir de textos del tipo `\"Etiqueta: Valor\"`.\n",
    "  4. Busca dentro de `detalles`:\n",
    "     - Claves relacionadas con **precio** (`\"precio\"`) y las convierte en entero.\n",
    "     - Claves de **tamaño/superficie** (e.g. `\"superficie\"`, `\"área de terreno\"`, `\"total edificado\"`) y las limpia con `clean_m2`.\n",
    "     - Claves de **habitaciones** (`\"dormitorios\"`, `\"habitaciones\"`) y las transforma en entero.\n",
    "  5. Intenta obtener la **ciudad** desde la clave `\"departamento\"` del diccionario; si no está, la deduce a partir del título con `parse_title_city`.\n",
    "  6. Si el precio aún no fue encontrado, busca directamente en el HTML un texto con el patrón `\"USD xxxx\"`.\n",
    "  7. Devuelve un diccionario con los datos estandarizados:\n",
    "     - `ciudad`\n",
    "     - `precio`\n",
    "     - `tamano` (m²)\n",
    "     - `habitaciones`\n",
    "     - `link` (URL del detalle)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:00:41.557404Z",
     "start_time": "2025-09-19T02:00:41.550151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_detail(detail_url):\n",
    "    soup = get_soup(detail_url)\n",
    "    title_el = soup.find([\"h1\",\"h2\"])\n",
    "    title = title_el.get_text(strip=True) if title_el else None\n",
    "\n",
    "    price = None\n",
    "    size_m2 = None\n",
    "    bedrooms = None\n",
    "    city = None\n",
    "\n",
    "    detalles = {}\n",
    "    for li in soup.select(\"li\"):\n",
    "        txt = li.get_text(\" \", strip=True)\n",
    "        if \":\" in txt:\n",
    "            k, v = [t.strip() for t in txt.split(\":\", 1)]\n",
    "            detalles[k.lower()] = v\n",
    "\n",
    "    for key in [\"precio\"]:\n",
    "        if key in detalles:\n",
    "            price = to_int(detalles[key])\n",
    "\n",
    "    for key in [\"tamaño de inmueble\", \"tamaño del inmueble\", \"superficie\", \"area de terreno\", \"área de terreno\", \"total edificado\", \"dificada\"]:\n",
    "        if key in detalles and size_m2 is None:\n",
    "            size_m2 = clean_m2(detalles[key])\n",
    "\n",
    "    for key in [\"dormitorios\", \"dormitorio\", \"habitaciones\"]:\n",
    "        if key in detalles and bedrooms is None:\n",
    "            bedrooms = to_int(detalles[key])\n",
    "\n",
    "    if \"departamento\" in detalles:\n",
    "        city = detalles[\"departamento\"].split(\",\")[0].strip()\n",
    "    if not city and title:\n",
    "        city = parse_title_city(title)\n",
    "\n",
    "    if price is None:\n",
    "        import re\n",
    "        usd_text = soup.find(string=re.compile(r\"USD\\s*[0-9\\.,]+\"))\n",
    "        if usd_text:\n",
    "            price = to_int(str(usd_text))\n",
    "\n",
    "    return {\n",
    "        \"ciudad\": city,\n",
    "        \"precio\": price,\n",
    "        \"tamano\": size_m2,\n",
    "        \"habitaciones\": bedrooms,\n",
    "        \"link\": detail_url\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4) Ejecución principal y guardado a `propiedades.json`"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La función **`scrape_por_ciudades`** orquesta el scraping por ciudad, combinando el listado y el detalle de cada propiedad, y guarda el resultado en un JSON.\n",
    "\n",
    "- **Parámetros:**\n",
    "  - `objetivos`: diccionario `{nombre_ciudad: url_listado}` a recorrer.\n",
    "  - `min_props`: mínimo de propiedades por ciudad a recopilar (por defecto 10).\n",
    "  - `max_pages`: máximo de páginas de listado a navegar por ciudad.\n",
    "  - `delay`: pausa (en segundos) entre requests de detalle para ser respetuosos con el sitio.\n",
    "\n",
    "- **Flujo de trabajo:**\n",
    "  1. Inicializa `data = {\"ciudades\": []}` para acumular resultados.\n",
    "  2. Itera sobre cada ciudad en `objetivos`:\n",
    "     - Llama a **`extract_cards_from_listing`** para obtener las “tarjetas” (título, precio textual si se encuentra y `detail_url`) desde la URL del listado, respetando `max_pages`.\n",
    "     - Recorre esas tarjetas y, evitando duplicados con `seen`, visita cada `detail_url`.\n",
    "     - En cada detalle, llama a **`parse_detail`** para extraer información estandarizada (`precio`, `tamano` en m², `habitaciones`, `link`, y opcionalmente `ciudad`).\n",
    "     - Si la página de detalle no expone la ciudad, la completa con el nombre del bloque (`nombre`).\n",
    "     - Agrega la propiedad al arreglo `props` y duerme `delay` segundos entre requests.\n",
    "     - Maneja errores por propiedad sin detener el scraping del resto (muestra el error y continúa).\n",
    "  3. Agrega al JSON final un bloque por ciudad con la forma:\n",
    "     ```json\n",
    "     {\n",
    "       \"nombre\": \"<Ciudad>\",\n",
    "       \"propiedades\": [ /* hasta min_props objetos */ ]\n",
    "     }\n",
    "     ```\n",
    "  4. Devuelve `data` con todas las ciudades."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:01:27.214875Z",
     "start_time": "2025-09-19T02:00:44.216880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrape_por_ciudades(objetivos, min_props=10, max_pages=5, delay=1.0):\n",
    "    data = {\"ciudades\": []}\n",
    "    import time\n",
    "    for nombre, list_url in objetivos.items():\n",
    "        print(f\"Recolectando en {nombre} ...\")\n",
    "        cards = extract_cards_from_listing(list_url, max_pages=max_pages)\n",
    "        print(f\"  {len(cards)} tarjetas encontradas\")\n",
    "        props = []\n",
    "        seen = set()\n",
    "        for card in cards:\n",
    "            if len(props) >= min_props:\n",
    "                break\n",
    "            detail = card[\"detail_url\"]\n",
    "            if detail in seen:\n",
    "                continue\n",
    "            seen.add(detail)\n",
    "            try:\n",
    "                info = parse_detail(detail)\n",
    "                if not info.get(\"ciudad\"):\n",
    "                    info[\"ciudad\"] = nombre\n",
    "                props.append({\n",
    "                    \"precio\": info.get(\"precio\"),\n",
    "                    \"tamano\": info.get(\"tamano\"),\n",
    "                    \"habitaciones\": info.get(\"habitaciones\"),\n",
    "                    \"link\": info.get(\"link\")\n",
    "                })\n",
    "                print(f\"    OK: {detail}\")\n",
    "                time.sleep(delay)\n",
    "            except Exception as e:\n",
    "                print(f\"    Error {detail}: {e}\")\n",
    "        data[\"ciudades\"].append({\n",
    "            \"nombre\": nombre,\n",
    "            \"propiedades\": props[:min_props]\n",
    "        })\n",
    "    return data\n",
    "\n",
    "OBJETIVOS = {\n",
    "    \"Montevideo\": \"https://inmobiliariauruguay.com/search-results/?type%5B%5D=urbana&states%5B%5D=montevideo\",\n",
    "    \"Maldonado\": \"https://inmobiliariauruguay.com/state/maldonado/\",\n",
    "    \"Canelones\": \"https://inmobiliariauruguay.com/state/canelones/\",\n",
    "}\n",
    "\n",
    "data = scrape_por_ciudades(OBJETIVOS, min_props=10, max_pages=5, delay=0.5)\n",
    "\n",
    "with open(\"propiedades.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    import json\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Archivo generado: propiedades.json\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recolectando en Montevideo ...\n",
      "  22 tarjetas encontradas\n",
      "    OK: https://inmobiliariauruguay.com/property/apartamento-de-2-dormitorios-en-buceo-montevideo/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-2-dormitorios-fondo-entrada-auto-con-porton-en-la-union-montevideo/\n",
      "    OK: https://inmobiliariauruguay.com/property/2-casa-en-venta-en-punta-de-manga-montevideo/\n",
      "    OK: https://inmobiliariauruguay.com/property/2-casas-en-un-mismo-padron-en-puntas-de-manga-montevideo/\n",
      "    OK: https://inmobiliariauruguay.com/property/galpon-a-la-venta-en-montevideo-2/\n",
      "    OK: https://inmobiliariauruguay.com/property/galpon-a-la-venta-en-montevideo/\n",
      "    OK: https://inmobiliariauruguay.com/property/apartamento-en-montevideo-9/\n",
      "    OK: https://inmobiliariauruguay.com/property/apartamento-en-montevideo-8/\n",
      "    OK: https://inmobiliariauruguay.com/property/hermoso-apartamento-en-montevideo/\n",
      "    OK: https://inmobiliariauruguay.com/property/apartamento-en-montevideo-6/\n",
      "Recolectando en Maldonado ...\n",
      "  45 tarjetas encontradas\n",
      "    OK: https://inmobiliariauruguay.com/property/campo-de-403has-ganadero-eolico-en-sierra-de-los-caracoles-maldonado/\n",
      "    OK: https://inmobiliariauruguay.com/property/apartamento-en-green-park-solanas-punta-del-este-maldonado/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-en-venta-3-dormitorios-amplio-terreno-en-aigua-maldonado/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-en-venta-5-dormitorios-2-banos-en-la-pedrera-rocha/\n",
      "    OK: https://inmobiliariauruguay.com/property/campo-ganadero-de-119-has-en-carape-maldonado/\n",
      "    OK: https://inmobiliariauruguay.com/property/terreno-en-venta-de-525-m2-en-playa-verde-maldonado/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-en-venta-en-playa-grande-maldonado/\n",
      "    OK: https://inmobiliariauruguay.com/property/campor-turistico-de-10-has-en-maldonado/\n",
      "    OK: https://inmobiliariauruguay.com/property/terreno-en-venta-de-10457-m2-en-punta-del-este-maldonado/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-de-4-dormitorios-y-demas-comodidades-en-aigua-maldonado/\n",
      "Recolectando en Canelones ...\n",
      "  45 tarjetas encontradas\n",
      "    OK: https://inmobiliariauruguay.com/property/campo-ganadero-con-casa-y-galpon-de-20-has-en-canelones/\n",
      "    OK: https://inmobiliariauruguay.com/property/chacra-de-18-has-turistica-en-canelones-a-3-km-de-soca/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-en-venta-4-dormitorios-en-balneario-san-luis-canelones/\n",
      "    OK: https://inmobiliariauruguay.com/property/campo-agricola-149-has-en-canelones/\n",
      "    OK: https://inmobiliariauruguay.com/property/apartamento-en-venta-en-el-centro-de-minas-2-dormitorios-y-2-banos-a-1-cuadra-de-plaza-libertad/\n",
      "    OK: https://inmobiliariauruguay.com/property/17-has-agricola-ganaderas-en-canelones-con-arroyo/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-2-dormitorios-amplio-tereno-en-san-luis-canelones/\n",
      "    OK: https://inmobiliariauruguay.com/property/campo-agricola-ganadero-11-has-en-canelones/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-en-venta-en-la-floresta-a-1-cuadra-de-la-playa/\n",
      "    OK: https://inmobiliariauruguay.com/property/casa-en-venta-en-solymar/\n",
      "Archivo generado: propiedades.json\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
